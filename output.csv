Unnamed: 0.1,Unnamed: 0,authors,aggregationType,Pages,publicationName,description,volume,title,subtypeDescription,citedby_count,PublishedDate
2,2,"Dharan Nidhin S,Gowtham Ramesh",Book,203-216,Lecture Notes in Networks and Systems,"We are proposing a personalized summarization model, which generates an abstractive summary of a random review based on the preference of a specific user. The summary will account the users preference on different aspects present in the review. We put forward a Personalized Key Information Guided Network (PKIGN) that pools both extractive and abstractive methods for summary generation. Specifically, keywords present in the review are extracted which are specific to that user, and these keywords are used as key information representation to guide the process of generating summaries. Additionally, Pointer-Guide mechanism is employed for obtaining long-term value for decoding. We evaluate our model on a new Trip-Advisor hotel review dataset, comprising of 140,874 reviews from 41,600 users. Combining the results from both human evaluation and quantitative analysis, it is seen that our model achieves better performance than existing models on personalized review summarization in case of hotel reviews.",336,Personalized Abstract Review Summarization Using Personalized Key Information-Guided Network,Conference Paper,0,2022-01-01
3,3,"Johnson Shanoop,Gowtham Ramesh,Nair Anand R",Book,153-167,Lecture Notes in Networks and Systems,"The growth of malware attacks has been phenomenal in the recent past. The COVID-19 pandemic has contributed to an increase in the dependence of a larger than usual workforce on digital technology. This has forced the anti-malware communities to build better software to mitigate malware attacks by detecting it before they wreak havoc. The key part of protecting a system from a malware attack is to identify whether a given file/software is malicious or not. Ransomware attacks are time-sensitive as they must be stopped before the attack manifests as the damage will be irreversible once the attack reaches a certain stage. Dynamic analysis employs a great many methods to decipher the way ransomware files behave when given a free rein. But, there still exists a risk of exposing the system to malicious code while doing that. Ransomware that can sense the analysis environment will most certainly elude the methods used in dynamic analysis. We propose a static analysis method along with machine learning for classifying the ransomware using opcodes extracted by disassemblers. By selecting the most appropriate feature vectors through the tf-idf feature selection method and tuning the parameters that better represent each class, we can increase the efficiency of the ransomware classification model. The ensemble learning-based model implemented on top of N-gram sequence of static opcode data was found to improve the performance significantly in comparison to RF, SVN, LR, and GBDT models when tested against a dataset consisting of live encrypting ransomware samples that had evasive technique to dodge dynamic malware analysis.",336,Ensemble Model Ransomware Classification: A Static Analysis-based Approach,Conference Paper,3,2022-01-01
5,5,"Gowtham Ramesh,Sanjay S P,Shandilya Shishir Kumar,Sountharrajan S",Journal,1-11,International Journal of Web-Based Learning and Teaching Technologies,"The intelligent personal assistant system is designed to support the individual researchers to enhance their quality of the research through the natural language interface. Specifically, this system automatically provides intrinsic details about the importance of the topic of discussion using the timeline analysis. The results generated by the system help the researchers to understand the preference of the global researchers in the specific research field. This system primarily identifies the core topic of the discussion from the user's presentation. Further, the importance of the topic is calculated based on the research articles published over three decades in the related field. The experimental results confirm that the proposed method accurately identifies whether the research topic the user presented is HOT.",16,A cognitive personal assistant system to enhance the individual-centric research capabilities,Article,0,2021-07-01
7,7,"Ravikumar Sourav,Vinod Dayanand,Gowtham Ramesh,Pulari Sini Raj,Mathi Senthilkumar",Journal,6291-6298,Journal of Intelligent and Fuzzy Systems,"Human-Elephant Conflict (HEC) and its mitigation have always been a serious conservation issue in India. It occurs mainly due to the encroachment of forests by humans as part of societal development. Consequently, these human settlements are highly affected by the intrusion of wild elephants as they cause extensive crop-raiding, injuries and even death in many cases. HEC is a growing problem in rural areas of India which shares a border with forests and other elephant habitats. Based on the studies, it is very explicit that HEC is an important conservation issue which affects the peaceful co-existence of both humans and elephants near the forest areas. The desirable solution for this problem would be to facilitate co-existence among humans and elephants, but this often fails because of technical difficulties. Hence, this paper presents an end-to-end technological solution to facilitate smoother coexistence of humans and elephants. The proposed work deploys a live video surveillance system along with deep learning strategies to effectively detect the presence of elephants. From the numerical analysis, it is revealed that the post-training accuracy of the deep learning model used in the proposed approach is evaluated at 98.7% and outperforms an an out-of-the-box image detector. The layered approach used in the proposed work improves resource management which is a major bottleneck in real-time deployment scenarios.",38,A layered approach to detect elephants in live surveillance video streams using convolution neural networks,Conference Paper,4,2020-01-01
9,9,"Alampalli Ramu Nikhil,Bandarupalli Mohana Sai,Nekkanti Manoj Sri Surya, Gowtham Ramesh",Book,1-10,Lecture Notes on Data Engineering and Communications Technologies,"The past two decades have witnessed the significant proliferation of technologies, which has laid a strong foundation for the scientific research in different fields. However, the changes across every field have also created new challenges related to the management of large chunks of data present in the process of converting, storing, searching and providing the user with relevant data. Extracting and transforming the data from one form to another remains as an important task in the current era. It becomes challenging when we focus on the particular extraction instances. Finding the proper research paper from the huge number of papers that includes navigation through the data is not an easy task. It includes huge amount of time search to provide the user with the most appropriate scientific paper of search. This paper concentrates on the extraction of problem statement from one research paper and will be further used to find the related papers. The use of phrases makes the search to considerably reduce the number of search across the Internet and at the same time, it yields a high performance.",38,Summarization of Research Publications Using Automatic Extraction,Book Chapter,6,2019-05-15
10,10,"Menen Anjalee,Gowtham Ramesh",Journal,28-31,International Journal of Recent Technology and Engineering,"Cyber security protects the system from unauthorized access and destruction of data. The intention is to provide security to the system by blocking attackers. Malware or malicious software is any kind of program which is developed with the aim of doing harm to victims data. Viruses, worms, Trojan horses, Ransomware, and spyware are different types of malware. When malicious software enters into the system, it will encrypt the user data, deletes or modifies the data. This type of software also used to steal the user data. Ransomware is one of the types of malware which was developed with the intention of getting money from the victims. When Ransomware starts executing in our system, it will start encrypting, deleting and modifying files. The user will get decryption key only after paying the claimed money. Many have found some solutions for detecting some specific Ransomware. The existing technique includes Static based technique which uses signature analysis which can only detect known Ransomware since it compares the extracted code snippet of the target executable with the database of known malware samples. The existing technique is based on the known input and known output and can only detect known Ransomware samples. In this paper we have proposed an efficient Ransomware detection system based on the analysis of behavior with the help of machine learning technique. In the proposed technique, we analyzed the possible behavior of Ransomware based on the changes to users files, addition of registry key, stopping the active processes. Based on this behavior, the decision is made using Machine learning technique.",7,An efficient ransomware detection system,Article,1,2019-05-01
11,11,"Krishna N Kaarthik,Prasanth M,Gowtham Ramesh,Karthic S,Mini K M",Conference,23816-23823,Materials Today: Proceedings,"The need for standard construction materials is increasing each day due to the pressure faced by construction sectors for achieving an enhanced growth and sustainable development in construction. These have made the developers go for different materials that can be used as an alternate material in construction. In the present work, natural fibers namely coir and sisal fiber are chosen to improve the properties of the concrete. Plain concrete properties are used as a reference to evaluate the effectiveness of this natural fiber reinforced concrete. The objective of this project is to determine the optimum level of coir fiber content for effective enhancement in ductile properties of concrete and on the usage of sisal fiber in increasing the strength properties of concrete.",5,Enhancement of properties of concrete using natural fibers,Conference Paper,63,2019-02-01
12,12,"Desul Sudarsana,Madurai Meenachi N,Venkatesh Thejas,Gunta Vijitha,Gowtham Ramesh,Sai Baba Magapu",Journal,2-15,Electronic Library,"Purpose: Ontology of a domain mainly consists of a set of concepts and their semantic relations. It is typically constructed and maintained by using ontology editors with substantial human intervention. It is desirable to perform the task automatically, which has led to the development of ontology learning techniques. One of the main challenges of ontology learning from the text is to identify key concepts from the documents. A wide range of techniques for key concept extraction have been proposed but are having the limitations of low accuracy, poor performance, not so flexible and applicability to a specific domain. The propose of this study is to explore a new method to extract key concepts and to apply them to literature in the nuclear domain. Design/methodology/approach: In this article, a novel method for key concept extraction is proposed and applied to the documents from the nuclear domain. A hybrid approach was used, which includes a combination of domain, syntactic name entity knowledge and statistical based methods. The performance of the developed method has been evaluated from the data obtained using two out of three voting logic from three domain experts by using 120 documents retrieved from SCOPUS database. Findings: The work reported pertains to extracting concepts from the set of selected documents and aids the search for documents relating to given concepts. The results of a case study indicated that the method developed has demonstrated better metrics than Text2Onto and CFinder. The method described has the capability of extracting valid key concepts from a set of candidates with long phrases. Research limitations/implications: The present study is restricted to literature coming out in the English language and applied to the documents from nuclear domain. It has the potential to extend to other domains also. Practical implications: The work carried out in the current study has the potential of leading to updating International Nuclear Information System thesaurus for ontology in the nuclear domain. This can lead to efficient search methods. Originality/value: This work is the first attempt to automatically extract key concepts from the nuclear documents. The proposed approach will address and fix the most of the problems that are existed in the current methods and thereby increase the performance.",37,Method for automatic key concepts extraction: Application to documents in the domain of nuclear reactors,Article,9,2018-01-01
13,13,"Gowtham Ramesh,Mathi Senthilkumar,Pulari Sini Raj,Krishnamoorthy Vidya",Conference,2284-2288,2017 International Conference on Advances in Computing Communications and Informatics ICACCI 2017,"Human-elephant conflict is a frontline conservation issue in the world. The loss and fragmentation of elephant's habitat owing to the increased human encroachment leads to a notable conservation issue. It raises the need for a non-invasive and efficient solution for the mitigation of human-elephant conflicts. Consequently, deploying a vision based surveillance method in the real time environment can prove to be significantly useful to provide the warnings well in advance thereby reducing the human elephant conflict. In this paper, a method for the identification of elephant as an object using image processing is proposed. The method dynamically learns from the trained images with different backgrounds, lighting conditions. Further, it classifies the input image based on the features of color and texture. The outcomes demonstrate that the proposed method effectively deals with the detection of elephants in near and far distances, cluttered and occluded environment.",2017,An automated vision-based method to detect elephants for mitigation of human-elephant conflicts,Conference Paper,12,2018-01-01
14,14,"Sripadh Thota,Gowtham Ramesh",Book,437-446,Lecture Notes in Computational Vision and Biomechanics,"Personalization is an emerging topic in the field of Research paper recommender systems and academic research. It is a technique to creative and efficient user profiles to achieve improved recommendations. Our work proposes a new user model to understand user behavior for personalization. This model initially extracts keywords based on the online behaviour of the user. The subsequent steps include concept extraction and user profile ontology construction to derive inferences and define relationships. The suggested model clearly depicts hierarchical ordering of the users long-term and current research interests. Furthermore, the adoption of our model contributes to improvement of recommendations.",28,Personalized research paper recommender system,Book Chapter,5,2018-01-01
15,15,"Sruthi M,Pulari Sini Raj,Gowtham Ramesh",Book,55-67,Lecture Notes in Computational Vision and Biomechanics,"Recommender systems have changed its purview from prediction accuracy oriented to finding more relevant and useful recommendations to user. Usefulness of items are different in different applications. This paper summarizes the works that have been done in this direction. Personalization, context awareness, multiple objectives of recommendations and evaluation metrics are reviewed in this paper.",28,Comprehensive study on usage of multi objectives in recommender systems,Book Chapter,1,2017-12-02
16,16,"Ramesh Gowtham,Selvakumar Kirubakara,Venugopal Archana",Journal,1244-1260,Behaviour and Information Technology,"Phishing is a fraudulent scheme to steal a users personal and confidential information by masking as a trustworthy entity in the electronic commerce. Phishers lure online users to visit their fake webpages and capture the users sensitive financial information. The current anti-phishing technique focuses on determining the legitimacy of the webpages that the user visits, and it alerts users with a phishing label when a webpage is found to have suspicious activity. Most of the times, however, these warnings are ignored by the users as there is no significant information present in the alerts except for the phishing label. The method proposed in this paper addresses the aforementioned lacunae by generating a coherent and complete explanation in the natural language text for the anti-phishing systems decision. The explanation includes the phishing label along with information to establish why such a decision has been taken. This would, in turn, contribute to the users enhanced understanding of the threat and also strengthens the users trust in the system. It is quite evident from the pilot evaluation, which involved 50 users, that the proposed methodology significantly improves the users understanding of the phishing label and strengthens their trust in the system.",36,Intelligent explanation generation system for phishing webpages by employing an inference system,Article,9,2017-11-30
17,17,"Ramesh Gowtham,Gupta Jithendranath,Gamya P G",Journal,75-84,Journal of Information Security and Applications,"Phishing is the act of stealing personal information from the online users by impersonating as a statutory source in the cyberspace. Phishers often bait online users to visit their forged webpages to acquire users sensitive information. Most of the anti-phishing techniques today, endeavor to identify the legitimacy of the webpages the user visits and warn them with a phishing label when the webpage is a phish. But, these warnings generated by the anti-phishing tools are generic and does not provide any assistance for the users to safely navigate to the legitimate webpages. Any anti-phishing technique will be incomplete and incompetent without having a victimized domain identification in place. The method proposed in this paper addresses this lacuna by automatically identifying the victimized domain (target domain) of every successfully distinguished phishing webpage. This method initially identifies the possible target domains of the webpage by analyzing the feign relationships which exist between the webpage and its associated domains through the in-degree link associations. Further, a novel Target Validation (TVD) algorithm is used to ensure the correctness of the identified target domain which in turn helps in reducing the false target predictions of the system. The legitimacy of the webpage is further confirmed using the identified target domain. The experiment results show that this method is efficient in protecting users from the online identity attacks and also in identifying victimized domain with over 99% accuracy.",35,Identification of phishing webpages and its target domains by analyzing the feign relationship,Article,19,2017-08-01
18,18,"Venugopal Archana, Gowtham Ramesh",Journal,16953-16960,International Journal of Applied Engineering Research,"Ontology verbalization is a process of converting the logical content of ontologies represented in the Web Ontology Language (OWL) into human understandable natural languages such as English. But, because of the ambiguous and complex nature of the natural languages, it is not directly suitable for verbalization. Controlled Natural Languages (CNLs) are derived from natural languages by applying restrictions and it can be further used for ontology verbalization and authoring. It helps the non-logicians to easily access the OWL ontologies. There are various controlled natural languages that can be used for both ontology authoring and verbalization. Each of the CNL has its own advantages and disadvantages. They overlap in some of the features, while differs widely in some other. The common goal of all the controlled natural languages is to make the OWL statements and the ontologies easily understandable for the users with little or no formal training. This paper focuses on comparing four predominantly used controlled natural languages such as Attempto Controlled English (ACE), Rabbit, Sydney OWL Syntax (SOS) and OWL Simplified English (OSE) with respect to simplicity, clearness, naturalness, and expressivity.",10,A study on verbalization of OWL axioms using controlled natural language,Article,9,2015-01-01
19,19,"Gowtham Ramesh,Krishnamurthi Ilango",Journal,1051-1068,Cluster Computing,"Phishing is web based criminal activity of making innocent online users to reveal sensitive information into fake web sites. Such fake web sites lead to fraudulent charges against individuals and corporations. Phishers have a lot of methods to design and host phished web pages, so in reality there cannot be a single solution that can help us combat phishing. As technology advances, the phishing techniques being used are also getting advanced and hence it demands the anti-phishing techniques also to be upgraded and the new techniques are to be included along with the existing methods. But most of the anti-phishing techniques today do not satisfy these criteria. In this paper, we propose service oriented three-layer architecture model for detecting and identifying phishing web sites as it overcomes the shortcomings of existing anti-phishing solutions. This model enables us to separate the user interface layer from the anti-phishing components layer. This is done through web service middleware layer, which provides us with the freedom of building our own anti-phishing components layer in an efficient and flexible way, independent of other layers. Anti-phishing components layer provides a set of reusable components to convert webpage into feature vectors using finest heuristic methods and external repositories of information. The feature vectors act as an input to trained support vector machine classifier to generate phishing label which determines whether a webpage is legitimate or a phishing page. This when experimented, displayed the significance and importance of three-layered architecture model along with combination of heuristics in detection of phishing webpage. This results in high accuracy of 99 % with less than 1 % of false positive rate. © 2013 Springer Science+Business Media New York.",17,PhishTackle-a web services architecture for anti-phishing,Article,14,2014-02-01
20,20,"Gowtham Ramesh,Krishnamurthi Ilango,Kumar K Sampath Sree",Journal,12-22,Decision Support Systems,"Phishing is a fraudulent act to acquire sensitive information from unsuspecting users by masking as a trustworthy entity in an electronic commerce. Several mechanisms such as spoofed e-mails, DNS spoofing and chat rooms which contain links to phishing websites are used to trick the victims. Though there are many existing anti-phishing solutions, phishers continue to lure the victims. In this paper, we present a novel approach that not only overcomes many of the difficulties in detecting phishing websites but also identifies the phishing target that is being mimicked. We have proposed an anti-phishing technique that groups the domains from hyperlinks having direct or indirect association with the given suspicious webpage. The domains gathered from the directly associated webpages are compared with the domains gathered from the indirectly associated webpages to arrive at a target domain set. On applying Target Identification (TID) algorithm on this set, we zero-in the target domain. We then perform third-party DNS lookup of the suspicious domain and the target domain and on comparison we identify the legitimacy of the suspicious page. © 2014 Elsevier B.V.",61,An efficacious method for detecting phishing webpages through target domain identification,Article,99,2014-01-01
21,21,"Gowtham Ramesh,Krishnamurthi Ilango",Journal,23-37,Computers and Security,"Phishing is a web-based criminal act. Phishing sites lure sensitive information from naive online users by camouflaging themselves as trustworthy entities. Phishing is considered an annoying threat in the field of electronic commerce. Due to the short lifespan of phishing webpages and the rapid advancement of phishing techniques, maintaining blacklists, white-lists or employing solely heuristics-based approaches are not particularly effective. The impact of phishing can be largely mitigated by adopting a suitable combination of all these techniques. In this study, the characteristics of legitimate and phishing webpages were investigated in depth, and based on this analysis, we proposed heuristics to extract 15 features from such webpages. These heuristic results were fed as an input to a trained machine learning algorithm to detect phishing sites. Before applying heuristics to the webpages, we used two preliminary screening modules in this system. The first module, the preapproved site identifier, checks webpages against a private white-list maintained by the user, and the second module, the Login Form Finder, classifies webpages as legitimate when there are no login forms present. These modules help to reduce superfluous computation in the system and in addition reducing the rate of false positives without compromising on the false negatives. By using all of these modules, we are able to classify webpages with 99.8% precision and a 0.4% of false positive rate. The experimental results indicate that this method is efficient for protecting users from online identity attacks. © 2013 Elsevier Ltd. All rights reserved.",40,A comprehensive and efficacious architecture for detecting phishing webpages,Article,88,2014-01-01
